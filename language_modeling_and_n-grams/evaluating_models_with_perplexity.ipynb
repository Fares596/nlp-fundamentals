{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17b6e1d",
   "metadata": {},
   "source": [
    "## Evaluating Language Models with Perplexity\n",
    "\n",
    "**Perplexity** is one of the most common metrics used to evaluate language models.  \n",
    "It measures how well a model predicts a sequence of words â€” in other words, how *confident* or *uncertain* the model is when generating text.\n",
    "\n",
    "A **lower perplexity** means the model assigns higher probabilities to the correct words (better performance),  \n",
    "while a **higher perplexity** indicates greater uncertainty or poorer predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Definition\n",
    "\n",
    "Given a test sequence of $N$ words $w_1, w_2, \\dots, w_N$ and a language model that assigns a conditional probability $P(w_i \\mid w_{1:i-1})$ to each word, the **perplexity** is defined as\n",
    "\n",
    "$ \\text{Perplexity} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(w_i \\mid w_{1:i-1})} $, which can also be expressed in an equivalent multiplicative form:\n",
    "$ \\text{Perplexity} = \\left( \\prod_{i=1}^{N} \\frac{1}{P(w_i \\mid w_{1:i-1})} \\right)^{\\frac{1}{N}} $.\n",
    "\n",
    "Both formulations measure the average uncertainty of the model over the test sequence, and a lower value indicates that the model assigns higher probabilities to the correct words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "543ed7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "#Training text\n",
    "\n",
    "training_text = [\"Alice\", \"wonders\",\"what\",\"is\",\"happening\", \"in\", \"Wonderland\"]\n",
    "\n",
    "#Generate bigram counts and unigram counts\n",
    "bigram_counts = Counter(bigrams(training_text))\n",
    "unigram_counts = Counter(training_text)\n",
    "vocabulary_size = len(set(training_text)) #set bc we want to count each word only 1 time (useless here)\n",
    "\n",
    "#function to calculate bigram probabilities \n",
    "\n",
    "def bigram_probability (bigram, bigram_counts, unigram_counts, vocabulary_size, alpha = 1) :\n",
    "    \"\"\"Calculate the probability of a bigram using Laplace Smoothing\"\"\"\n",
    "    w1,w2 = bigram\n",
    "    numerator = bigram_counts[bigram] + alpha\n",
    "    denominator = unigram_counts[w1] +alpha * vocabulary_size\n",
    "    return numerator/denominator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c9652",
   "metadata": {},
   "source": [
    "## Testing the model \n",
    "\n",
    "To compute perplexity, we first need the probablities for all the bigrams in a given test sequence (using Laplace smoothing ensures no bigram has a zero probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e56267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the test text : 3.2813414240305514\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Function to calculate perplexity\n",
    "def calculate_perplexity(test_text, bigram_counts, unigram_counts, vocabulary_size, alpha =1) :\n",
    "    \"\"\"Calculate the perplexity of a test text using a bigram language model.\"\"\"\n",
    "\n",
    "    N = len(test_text)\n",
    "    log_probability_sum = 0\n",
    "\n",
    "    for i in range(1,N):\n",
    "        bigram = (test_text[i-1], test_text[i])\n",
    "        probability = bigram_probability(bigram, bigram_counts, unigram_counts, vocabulary_size, alpha)\n",
    "        log_probability_sum += np.log2(probability)\n",
    "    \n",
    "    perplexity = 2**(-log_probability_sum /N)\n",
    "    return perplexity\n",
    "\n",
    "#Test text (same as training, did this on purpose to compare with unseen words later)\n",
    "test_text = [\"Alice\", \"wonders\",\"what\",\"is\",\"happening\", \"in\", \"Wonderland\"] \n",
    "\n",
    "#Calculate perplexity\n",
    "perplexity = calculate_perplexity(test_text, bigram_counts, unigram_counts, vocabulary_size)\n",
    "print(f\"Perplexity of the test text : {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e658490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the unseen test text : 4.449605586254059\n"
     ]
    }
   ],
   "source": [
    "#Test with unseen sequence\n",
    "unseen_test_text = [\"Alice\", \"dreams\", \"about\", \"Wonderland\"]\n",
    "\n",
    "#Calculate perplexity for unseen sequence\n",
    "perplexity_unseen = calculate_perplexity(unseen_test_text, bigram_counts, unigram_counts, vocabulary_size)\n",
    "\n",
    "print(f\"Perplexity of the unseen test text : {perplexity_unseen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998904d0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As expected, perplexity is higher for the unseen sequence because bigram_probability returns lower probabilities than for the test_sequence which contains bigrams that are in the training sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
