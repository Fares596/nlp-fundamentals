{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52ce8eb",
   "metadata": {},
   "source": [
    "## Steps to Build a Naive Bayes Classifier\n",
    "\n",
    "### **Step 1 : Collect Data**\n",
    "First, we need a labeled dataset where each document (text) is tagged with its correct class label.  \n",
    "For example, in spam detection, emails would be labeled as either `\"spam\"` or `\"not spam\"`.\n",
    "\n",
    "### **Step 2 : Preprocess the Text**\n",
    "Before applying Naive Bayes, it's common to preprocess the text:\n",
    "- Tokenization\n",
    "- Lowercasing\n",
    "- Stopword Removal\n",
    "- Stemming/Lemmatization\n",
    "\n",
    "### **Step 3 : Calculate Probabilities**\n",
    "For each class, we need to estimate:  \n",
    "\n",
    "1. **Prior probability** $P(c)$ — probability of a class occurring in the dataset.  \n",
    "$$\n",
    "P(c) = \\frac{\\text{Number of documents in class } c}{\\text{Total number of documents}}\n",
    "$$\n",
    "\n",
    "2. **Conditional probabilities** $P(w \\mid c)$ — probability of each word $w$ given the class $c$.  \n",
    "Using the multinomial model with Laplace smoothing:\n",
    "$$\n",
    "P(w \\mid c) = \\frac{\\text{Count}(w \\text{ in class } c) + 1}{\\sum_{w' \\in V} \\text{Count}(w' \\text{ in class } c) + |V|}\n",
    "$$  \n",
    "where $V$ is the vocabulary.\n",
    "\n",
    "### **Step 4 : Apply Bayes Theorem**\n",
    "For a new document $d$ with words $w_1, w_2, \\dots, w_n$, compute the posterior probability for each class $c$:\n",
    "$$\n",
    "P(c \\mid d) \\propto P(c) \\prod_{i=1}^{n} P(w_i \\mid c)\n",
    "$$\n",
    "\n",
    "Then choose the class with the highest posterior probability:\n",
    "$$\n",
    "\\hat{c} = \\arg\\max_c P(c) \\prod_{i=1}^{n} P(w_i \\mid c)\n",
    "$$\n",
    "\n",
    "### **Step 5 : Predict the Class**\n",
    "For a given document, calculate the posterior probability for each class and classify the document as belonging to the class with the highest posterior probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb43a7",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca35130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probabilities: {'Animal': 0.5, 'Vehicle': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Count the number of documents in each class\n",
    "class_count = {\"Animal\" : 2, \"Vehicle\":2}\n",
    "\n",
    "#Calculate prior probabilities\n",
    "total_documents = sum(class_count.values())\n",
    "prior_prob = {label: count/total_documents for label, count in class_count.items()}\n",
    "\n",
    "print(\"Prior Probabilities:\", prior_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343c315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Probabilities: {'Animal': {'the': 0.15, 'cat': 0.1, 'chases': 0.1, 'mouse': 0.1, 'dog': 0.1, 'barks': 0.1, 'loudly': 0.1, 'car': 0.05, 'is': 0.05, 'fast': 0.05, 'truck': 0.05, 'large': 0.05}, 'Vehicle': {'the': 0.15, 'cat': 0.05, 'chases': 0.05, 'mouse': 0.05, 'dog': 0.05, 'barks': 0.05, 'loudly': 0.05, 'car': 0.1, 'is': 0.15, 'fast': 0.1, 'truck': 0.1, 'large': 0.1}}\n"
     ]
    }
   ],
   "source": [
    "#Define the vocabulary and word counts for each class\n",
    "vocabulary = [\"the\", \"cat\",\"chases\",\"mouse\",\"dog\",\"barks\",\"loudly\",\"car\",\"is\",\"fast\",\"truck\",\"large\"]\n",
    "word_count = {\n",
    "    \"Animal\": {\"the\":2, \"cat\":1, \"chases\":1, \"mouse\":1, \"dog\":1, \"barks\":1, \"loudly\":1},\n",
    "    \"Vehicle\": {\"the\":2, \"car\":1, \"is\":2, \"fast\":1, \"truck\":1, \"large\":1}\n",
    "}\n",
    "\n",
    "#Calculate conditional probabilities\n",
    "total_word_count = {\"Animal\":7, \"Vehicle\":7}\n",
    "cond_prob = {}\n",
    "\n",
    "for label in [\"Animal\", \"Vehicle\"]:\n",
    "    cond_prob[label]= {}\n",
    "    total_words = sum(word_count[label].values())\n",
    "\n",
    "    for word in vocabulary:\n",
    "        # Laplace smoothing: +1 in numerator, +|V| in denominator\n",
    "        cond_prob[label][word] = (word_count[label].get(word,0) + 1) / (total_words + len(vocabulary))\n",
    "\n",
    "print(\"Conditional Probabilities:\", cond_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a88ff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'the cat chases the mouse' -> Predicted class: Animal\n",
      "Sentence: 'the car is fast' -> Predicted class: Vehicle\n",
      "Sentence: 'the dog barks loudly' -> Predicted class: Animal\n",
      "Sentence: 'the truck is red' -> Predicted class: Vehicle\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Calculate posterior probabilities for a new document\n",
    "\n",
    "def predict_class(document, prior_prob, cond_prob):\n",
    "    words = document.split()\n",
    "    max_prob = -float(\"inf\")\n",
    "    best_class = None\n",
    "\n",
    "    for label in [\"Animal\", \"Vehicle\"]:\n",
    "        prob = np.log(prior_prob[label])\n",
    "        for word in words:\n",
    "            prob += np.log(cond_prob[label].get(word, 1e-5))\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            best_class = label\n",
    "    return best_class\n",
    "\n",
    "\n",
    "# Step 5: Test the model\n",
    "test_sentences = [\n",
    "    \"the cat chases the mouse\",   # Animal\n",
    "    \"the car is fast\",            # Vehicle\n",
    "    \"the dog barks loudly\",       # Animal\n",
    "    \"the truck is red\"          # Vehicle\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    predicted = predict_class(sentence, prior_prob, cond_prob)\n",
    "    print(f\"Sentence: '{sentence}' -> Predicted class: {predicted}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
